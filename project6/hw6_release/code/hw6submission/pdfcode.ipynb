{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7487353",
   "metadata": {},
   "source": [
    "# Layer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5ee145d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neural_networks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mabc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ABC, abstractmethod\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneural_networks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initialize_activation\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneural_networks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvolution\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad2d\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneural_networks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mweights\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initialize_weights\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'neural_networks'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Author: Sophia Sanborn, Sagnik Bhattacharya\n",
    "Institution: UC Berkeley\n",
    "Date: Spring 2020\n",
    "Course: CS189/289A\n",
    "Website: github.com/sophiaas, github.com/sagnibak\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from neural_networks.activations import initialize_activation\n",
    "from neural_networks.utils.convolution import pad2d\n",
    "from neural_networks.weights import initialize_weights\n",
    "from collections import OrderedDict\n",
    "\n",
    "from typing import Callable, List, Literal, Tuple, Union\n",
    "\n",
    "\n",
    "class Layer(ABC):\n",
    "    \"\"\"Abstract class defining the `Layer` interface.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.activation = None\n",
    "\n",
    "        self.n_in = None\n",
    "        self.n_out = None\n",
    "\n",
    "        self.parameters = {}\n",
    "        self.cache = {}\n",
    "        self.gradients = {}\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, z: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def clear_gradients(self) -> None:\n",
    "        self.cache = OrderedDict({a: [] for a, b in self.cache.items()})\n",
    "        self.gradients = OrderedDict(\n",
    "            {a: np.zeros_like(b) for a, b in self.gradients.items()}\n",
    "        )\n",
    "\n",
    "    def forward_with_param(\n",
    "        self, param_name: str, X: np.ndarray,\n",
    "    ) -> Callable[[np.ndarray], np.ndarray]:\n",
    "        \"\"\"Call the `forward` method but with `param_name` as the variable with\n",
    "        value `param_val`, and keep `X` fixed.\n",
    "        \"\"\"\n",
    "\n",
    "        def inner_forward(param_val: np.ndarray) -> np.ndarray:\n",
    "            self.parameters[param_name] = param_val\n",
    "            return self.forward(X)\n",
    "\n",
    "        return inner_forward\n",
    "\n",
    "    def _get_parameters(self) -> List[np.ndarray]:\n",
    "        return [b for a, b in self.parameters.items()]\n",
    "\n",
    "    def _get_cache(self) -> List[np.ndarray]:\n",
    "        return [b for a, b in self.cache.items()]\n",
    "\n",
    "    def _get_gradients(self) -> List[np.ndarray]:\n",
    "        return [b for a, b in self.gradients.items()]\n",
    "\n",
    "\n",
    "def initialize_layer(\n",
    "    name: str,\n",
    "    activation: str = None,\n",
    "    weight_init: str = None,\n",
    "    n_out: int = None,\n",
    "    kernel_shape: Tuple[int, int] = None,\n",
    "    stride: int = None,\n",
    "    pad: int = None,\n",
    "    mode: str = None,\n",
    "    keep_dim: str = \"first\",\n",
    ") -> Layer:\n",
    "    \"\"\"Factory function for layers.\"\"\"\n",
    "    if name == \"fully_connected\":\n",
    "        return FullyConnected(\n",
    "            n_out=n_out, activation=activation, weight_init=weight_init,\n",
    "        )\n",
    "\n",
    "    elif name == \"conv2d\":\n",
    "        return Conv2D(\n",
    "            n_out=n_out,\n",
    "            activation=activation,\n",
    "            kernel_shape=kernel_shape,\n",
    "            stride=stride,\n",
    "            pad=pad,\n",
    "            weight_init=weight_init,\n",
    "        )\n",
    "\n",
    "    elif name == \"pool2d\":\n",
    "        return Pool2D(kernel_shape=kernel_shape, mode=mode, stride=stride, pad=pad)\n",
    "\n",
    "    elif name == \"flatten\":\n",
    "        return Flatten(keep_dim=keep_dim)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Layer type {} is not implemented\".format(name))\n",
    "\n",
    "\n",
    "class FullyConnected(Layer):\n",
    "    \"\"\"A fully-connected layer multiplies its input by a weight matrix, adds\n",
    "    a bias, and then applies an activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, n_out: int, activation: str, weight_init=\"xavier_uniform\"\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_in = None\n",
    "        self.n_out = n_out\n",
    "        self.activation = initialize_activation(activation)\n",
    "\n",
    "        # instantiate the weight initializer\n",
    "        self.init_weights = initialize_weights(weight_init, activation=activation)\n",
    "\n",
    "    def _init_parameters(self, X_shape: Tuple[int, int]) -> None:\n",
    "        \"\"\"Initialize all layer parameters (weights, biases).\"\"\"\n",
    "        self.n_in = X_shape[1]\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        W = self.init_weights((self.n_in, self.n_out))\n",
    "        b = np.zeros((1, self.n_out))\n",
    "\n",
    "        self.parameters = OrderedDict({\"W\": W, \"b\": b})\n",
    "        self.cache: OrderedDict = OrderedDict()  # cache for backprop, stores the X and Z terms\n",
    "        self.gradients: OrderedDict = OrderedDict({\"W\": np.zeros((self.n_in, self.n_out)), \"b\": np.zeros((1, self.n_out))})  # parameter gradients initialized to zero\n",
    "                                           # MUST HAVE THE SAME KEYS AS `self.parameters`\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass: multiply by a weight matrix, add a bias, apply activation.\n",
    "        Also, store all necessary intermediate results in the `cache` dictionary\n",
    "        to be able to compute the backward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input matrix of shape (batch_size, input_dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a matrix of shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        # initialize layer parameters if they have not been initialized\n",
    "        if self.n_in is None:\n",
    "            self._init_parameters(X.shape)\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "        \n",
    "        # perform an affine transformation and activation\n",
    "        W = self.parameters[\"W\"]\n",
    "        b = self.parameters[\"b\"]\n",
    "        Z = (X @ W) + b\n",
    "        out = self.activation(Z)\n",
    "        \n",
    "        # store information necessary for backprop in `self.cache`\n",
    "        self.cache[\"X\"] = X\n",
    "        self.cache[\"Z\"] = Z\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dLdY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for fully connected layer.\n",
    "        Compute the gradients of the loss with respect to:\n",
    "            1. the weights of this layer (mutate the `gradients` dictionary)\n",
    "            2. the bias of this layer (mutate the `gradients` dictionary)\n",
    "            3. the input of this layer (return this)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dLdY  derivative of the loss with respect to the output of this layer\n",
    "              shape (batch_size, output_dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        derivative of the loss with respect to the input of this layer\n",
    "        shape (batch_size, input_dim)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "        # unpack the cache\n",
    "        X = self.cache[\"X\"]\n",
    "        Z = self.cache[\"Z\"]\n",
    "        W = self.parameters[\"W\"]\n",
    "        \n",
    "        # compute the gradients of the loss w.r.t. all parameters as well as the\n",
    "        # input of the layer\n",
    "\n",
    "        dZ = self.activation.backward(Z, dLdY)\n",
    "        dW = X.T @ dZ\n",
    "        db = (np.ones((1, dZ.shape[0])) @ dZ).reshape(self.n_out,)\n",
    "        dX = dZ @ W.T\n",
    "\n",
    "        # store the gradients in `self.gradients`\n",
    "        self.gradients[\"W\"] = dW\n",
    "        self.gradients[\"b\"] = db\n",
    "        # the gradient for self.parameters[\"W\"] should be stored in\n",
    "        # self.gradients[\"W\"], etc.\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return dX\n",
    "\n",
    "\n",
    "class Conv2D(Layer):\n",
    "    \"\"\"Convolutional layer for inputs with 2 spatial dimensions.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_out: int,\n",
    "        kernel_shape: Tuple[int, int],\n",
    "        activation: str,\n",
    "        stride: int = 1,\n",
    "        pad: str = \"same\",\n",
    "        weight_init: str = \"xavier_uniform\",\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_in = None\n",
    "        self.n_out = n_out\n",
    "        self.kernel_shape = kernel_shape\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "        self.activation = initialize_activation(activation)\n",
    "        self.init_weights = initialize_weights(weight_init, activation=activation)\n",
    "\n",
    "    def _init_parameters(self, X_shape: Tuple[int, int, int, int]) -> None:\n",
    "        \"\"\"Initialize all layer parameters and determine padding.\"\"\"\n",
    "        self.n_in = X_shape[3]\n",
    "\n",
    "        W_shape = self.kernel_shape + (self.n_in,) + (self.n_out,)\n",
    "        W = self.init_weights(W_shape)\n",
    "        b = np.zeros((1, self.n_out))\n",
    "\n",
    "        self.parameters = OrderedDict({\"W\": W, \"b\": b})\n",
    "        self.cache = OrderedDict({\"Z\": [], \"X\": []})\n",
    "        self.gradients = OrderedDict({\"W\": np.zeros_like(W), \"b\": np.zeros_like(b)})\n",
    "\n",
    "        if self.pad == \"same\":\n",
    "            self.pad = ((W_shape[0] - 1) // 2, (W_shape[1] - 1) // 2)\n",
    "        elif self.pad == \"valid\":\n",
    "            self.pad = (0, 0)\n",
    "        elif isinstance(self.pad, int):\n",
    "            self.pad = (self.pad, self.pad)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Pad mode found in self.pad.\")\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for convolutional layer. This layer convolves the input\n",
    "        `X` with a filter of weights, adds a bias term, and applies an activation\n",
    "        function to compute the output. This layer also supports padding and\n",
    "        integer strides. Intermediates necessary for the backward pass are stored\n",
    "        in the cache.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input with shape (batch_size, in_rows, in_cols, in_channels)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output feature maps with shape (batch_size, out_rows, out_cols, out_channels)\n",
    "        \"\"\"\n",
    "        if self.n_in is None:\n",
    "            self._init_parameters(X.shape)\n",
    "\n",
    "        W = self.parameters[\"W\"]\n",
    "        b = self.parameters[\"b\"]\n",
    "\n",
    "        kernel_height, kernel_width, in_channels, out_channels = W.shape\n",
    "        n_examples, in_rows, in_cols, in_channels = X.shape\n",
    "        kernel_shape = (kernel_height, kernel_width)\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "        # implement a convolutional forward pass\n",
    "        rows = int(1 + (in_rows + 2 * self.pad[0] - kernel_height) / self.stride)\n",
    "        cols = int(1 + (in_cols + 2 * self.pad[1] - kernel_width) / self.stride)\n",
    "        Z = np.zeros((n_examples, rows, cols, out_channels))\n",
    "        x_padded, p = pad2d(X, self.pad, self.kernel_shape, self.stride)\n",
    "\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                h = i * self.stride\n",
    "                w = j * self.stride\n",
    "                convolution = np.multiply(x_padded[:, h:h+kernel_height, w:w+kernel_width, :, None], W)\n",
    "                Z[:, i, j, :] = np.sum(convolution, axis = (1, 2, 3)) + b\n",
    "        out = self.activation(Z)\n",
    "\n",
    "        # cache any values required for backprop\n",
    "        self.cache[\"Z\"] = Z\n",
    "        self.cache[\"X\"] = X\n",
    "        ### END YOUR CODE ###\n",
    "        return out\n",
    "\n",
    "    def backward(self, dLdY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for conv layer. Computes the gradients of the output\n",
    "        with respect to the input feature maps as well as the filter weights and\n",
    "        biases.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dLdY  derivative of loss with respect to output of this layer\n",
    "              shape (batch_size, out_rows, out_cols, out_channels)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        derivative of the loss with respect to the input of this layer\n",
    "        shape (batch_size, in_rows, in_cols, in_channels)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###  \n",
    "        Z = self.cache[\"Z\"]\n",
    "        X = self.cache[\"X\"]\n",
    "        W = self.parameters[\"W\"]\n",
    "        b = self.parameters[\"b\"]\n",
    "        dx_padded, p = pad2d(X, self.pad, self.kernel_shape, self.stride)\n",
    "\n",
    "        kernel_height, kernel_width, in_channels, out_channels = W.shape\n",
    "        n_examples, in_rows, in_cols, in_channels = X.shape\n",
    "        kernel_shape = (kernel_height, kernel_width)\n",
    "\n",
    "        dLdY = self.activation.backward(Z, dLdY)\n",
    "        dX = np.zeros_like(dx_padded)\n",
    "        dLdW = np.zeros_like(W)\n",
    "        dLdb = dLdY.sum(axis=(1, 2, 3)).reshape((1, -1))\n",
    "\n",
    "        for i in range(dLdY.shape[1]):\n",
    "            for j in range(dLdY.shape[2]):\n",
    "                h = i * self.stride\n",
    "                w = j * self.stride\n",
    "\n",
    "                for f in range(out_channels):\n",
    "                    X_slice = dX[:, h:h+kernel_height, w:w+kernel_width, :]\n",
    "                    dL_slice = dLdY[:,i, j, f][:, None, None, None]\n",
    "                    dLdW[:, :, :, f] +=  np.sum(dL_slice * X_slice)\n",
    "                \n",
    "                for n in range(n_examples):\n",
    "                    dX[n, h:h+kernel_height, w:w+kernel_width, :] += np.sum(W * dLdY[n, i, j,:], axis = 3)\n",
    "        \n",
    "        # un-pad dX\n",
    "        dLdX = dX[:, p[1]:-p[1], p[2]:-p[2], :]\n",
    "        \n",
    "        self.gradients[\"X\"] = dLdX\n",
    "        self.gradients[\"W\"] = dLdW\n",
    "        self.gradients[\"b\"] = dLdb\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return dLdX\n",
    "\n",
    "class Pool2D(Layer):\n",
    "    \"\"\"Pooling layer, implements max and average pooling.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_shape: Tuple[int, int],\n",
    "        mode: str = \"max\",\n",
    "        stride: int = 1,\n",
    "        pad: Union[int, Literal[\"same\"], Literal[\"valid\"]] = 0,\n",
    "    ) -> None:\n",
    "\n",
    "        if type(kernel_shape) == int:\n",
    "            kernel_shape = (kernel_shape, kernel_shape)\n",
    "\n",
    "        self.kernel_shape = kernel_shape\n",
    "        self.stride = stride\n",
    "\n",
    "        if pad == \"same\":\n",
    "            self.pad = ((kernel_shape[0] - 1) // 2, (kernel_shape[1] - 1) // 2)\n",
    "        elif pad == \"valid\":\n",
    "            self.pad = (0, 0)\n",
    "        elif isinstance(pad, int):\n",
    "            self.pad = (pad, pad)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Pad mode found in self.pad.\")\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode == \"max\":\n",
    "            self.pool_fn = np.max\n",
    "            self.arg_pool_fn = np.argmax\n",
    "        elif mode == \"average\":\n",
    "            self.pool_fn = np.mean\n",
    "\n",
    "        self.cache = {\n",
    "            \"out_rows\": [],\n",
    "            \"out_cols\": [],\n",
    "            \"X_pad\": [],\n",
    "            \"p\": [],\n",
    "            \"pool_shape\": [],\n",
    "        }\n",
    "        self.parameters = {}\n",
    "        self.gradients = {}\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass: use the pooling function to aggregate local information\n",
    "        in the input. This layer typically reduces the spatial dimensionality of\n",
    "        the input while keeping the number of feature maps the same.\n",
    "\n",
    "        As with all other layers, please make sure to cache the appropriate\n",
    "        information for the backward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input array of shape (batch_size, in_rows, in_cols, channels)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pooled array of shape (batch_size, out_rows, out_cols, channels)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        n_examples, in_rows, in_cols, in_channels = X.shape\n",
    "        kernel_height, kernel_width = self.kernel_shape\n",
    "\n",
    "        rows = int(1 + (in_rows + 2 * self.pad[0] - kernel_height) / self.stride)\n",
    "        cols = int(1 + (in_cols + 2 * self.pad[1] - kernel_width) / self.stride)\n",
    "\n",
    "        # implement the forward pass\n",
    "        X_pool = np.zeros((n_examples, rows, cols, in_channels))\n",
    "        x_padded, p = pad2d(X, self.pad, self.kernel_shape, self.stride)\n",
    "\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                h = i * self.stride\n",
    "                w = j * self.stride\n",
    "                slice = x_padded[:, h:h+kernel_height, w:w+kernel_width, :]\n",
    "                X_pool[:, i, j, :] = self.pool_fn(slice, axis = (1, 2))\n",
    "\n",
    "        # cache any values required for backprop\n",
    "        self.cache[\"Z\"] = X_pool\n",
    "        self.cache[\"X\"] = X\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return X_pool\n",
    "\n",
    "    def backward(self, dLdY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for pooling layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dLdY  gradient of loss with respect to the output of this layer\n",
    "              shape (batch_size, out_rows, out_cols, channels)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient of loss with respect to the input of this layer\n",
    "        shape (batch_size, in_rows, in_cols, channels)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "        X = self.cache[\"X\"]\n",
    "        Z = self.cache[\"Z\"]\n",
    "\n",
    "        batch_size, out_rows, out_cols, out_channels = Z.shape\n",
    "        n_examples, in_rows, in_cols, in_channels = X.shape\n",
    "        kernel_height, kernel_width = self.kernel_shape\n",
    "\n",
    "        x_padded, p = pad2d(X, self.pad, self.kernel_shape, self.stride)\n",
    "\n",
    "        dLdX = np.zeros_like(x_padded)\n",
    "\n",
    "        for i in range(out_rows):\n",
    "            for j in range(out_cols):\n",
    "                if self.mode == \"max\":\n",
    "\n",
    "                    h = i * self.stride\n",
    "                    w = j * self.stride\n",
    "\n",
    "                    slice = x_padded[:, h:h+kernel_height, w:w+kernel_width, :]\n",
    "                    # finding index with of max value\n",
    "                    max_index = self.arg_pool_fn(slice.reshape(n_examples, kernel_height * kernel_width, in_channels), axis = 1)\n",
    "                    mask = np.zeros_like(slice.reshape(n_examples, kernel_height * kernel_width, in_channels))\n",
    "                    mask[np.arange(n_examples), max_index] = 1\n",
    "                    mask = mask.reshape(n_examples, kernel_height, kernel_width, in_channels)\n",
    "                    dLdX[:, h:h+kernel_height, w:w+kernel_width, :] += mask * dLdY[:, i, j, :]\n",
    "\n",
    "                elif self.mode == \"average\":\n",
    "                    pass\n",
    "                else:\n",
    "                    pass\n",
    "                    \n",
    "\n",
    "        # perform a backward pass\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "        return dLdX\n",
    "\n",
    "class Flatten(Layer):\n",
    "    \"\"\"Flatten the input array.\"\"\"\n",
    "\n",
    "    def __init__(self, keep_dim: str = \"first\") -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.keep_dim = keep_dim\n",
    "        self._init_params()\n",
    "\n",
    "    def _init_params(self):\n",
    "        self.X = []\n",
    "        self.gradients = {}\n",
    "        self.parameters = {}\n",
    "        self.cache = {\"in_dims\": []}\n",
    "\n",
    "    def forward(self, X: np.ndarray, retain_derived: bool = True) -> np.ndarray:\n",
    "        self.cache[\"in_dims\"] = X.shape\n",
    "\n",
    "        if self.keep_dim == -1:\n",
    "            return X.flatten().reshape(1, -1)\n",
    "\n",
    "        rs = (X.shape[0], -1) if self.keep_dim == \"first\" else (-1, X.shape[-1])\n",
    "        return X.reshape(*rs)\n",
    "\n",
    "    def backward(self, dLdY: np.ndarray) -> np.ndarray:\n",
    "        in_dims = self.cache[\"in_dims\"]\n",
    "        dX = dLdY.reshape(in_dims)\n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f50ca7",
   "metadata": {},
   "source": [
    "# Activation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91109fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Sophia Sanborn\n",
    "Institution: UC Berkeley\n",
    "Date: Spring 2020\n",
    "Course: CS189/289A\n",
    "Website: github.com/sophiaas\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class Activation(ABC):\n",
    "    \"\"\"Abstract class defining the common interface for all activation methods.\"\"\"\n",
    "\n",
    "    def __call__(self, Z):\n",
    "        return self.forward(Z)\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, Z):\n",
    "        pass\n",
    "\n",
    "\n",
    "def initialize_activation(name: str) -> Activation:\n",
    "    \"\"\"Factory method to return an Activation object of the specified type.\"\"\"\n",
    "    if name == \"linear\":\n",
    "        return Linear()\n",
    "    elif name == \"sigmoid\":\n",
    "        return Sigmoid()\n",
    "    elif name == \"tanh\":\n",
    "        return TanH()\n",
    "    elif name == \"arctan\":\n",
    "        return ArcTan()\n",
    "    elif name == \"relu\":\n",
    "        return ReLU()\n",
    "    elif name == \"softmax\":\n",
    "        return SoftMax()\n",
    "    else:\n",
    "        raise NotImplementedError(\"{} activation is not implemented\".format(name))\n",
    "\n",
    "\n",
    "class Linear(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for f(z) = z.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z  input pre-activations (any shape)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        f(z) as described above applied elementwise to `Z`\n",
    "        \"\"\"\n",
    "        return Z\n",
    "\n",
    "    def backward(self, Z: np.ndarray, dY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for f(z) = z.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z   input to `forward` method\n",
    "        dY  derivative of loss w.r.t. the output of this layer\n",
    "            same shape as `Z`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        derivative of loss w.r.t. input of this layer\n",
    "        \"\"\"\n",
    "        return dY\n",
    "\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for sigmoid function:\n",
    "        f(z) = 1 / (1 + exp(-z))\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z  input pre-activations (any shape)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        f(z) as described above applied elementwise to `Z`\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        return ...\n",
    "\n",
    "    def backward(self, Z: np.ndarray, dY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for sigmoid.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z   input to `forward` method\n",
    "        dY  derivative of loss w.r.t. the output of this layer\n",
    "            same shape as `Z`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        derivative of loss w.r.t. input of this layer\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        return ...\n",
    "\n",
    "\n",
    "class TanH(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for f(z) = tanh(z).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z  input pre-activations (any shape)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        f(z) as described above applied elementwise to `Z`\n",
    "        \"\"\"\n",
    "        return 2 / (1 + np.exp(-2 * Z)) - 1\n",
    "\n",
    "    def backward(self, Z: np.ndarray, dY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for f(z) = tanh(z).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z   input to `forward` method\n",
    "        dY  derivative of loss w.r.t. the output of this layer\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        derivative of loss w.r.t. input of this layer\n",
    "        \"\"\"\n",
    "        fn = self.forward(Z)\n",
    "        return dY * (1 - fn ** 2)\n",
    "\n",
    "\n",
    "class ReLU(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for relu activation:\n",
    "        f(z) = z if z >= 0\n",
    "               0 otherwise\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z  input pre-activations (any shape)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        f(z) as described above applied elementwise to `Z`\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def backward(self, Z: np.ndarray, dY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for relu activation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z   input to `forward` method\n",
    "        dY  derivative of loss w.r.t. the output of this layer\n",
    "            same shape as `Z`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        derivative of loss w.r.t. input of this layer\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        mask = np.where(Z >= 0, 1, 0)\n",
    "        return dY * mask\n",
    "\n",
    "\n",
    "class SoftMax(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for softmax activation.\n",
    "        Hint: The naive implementation might not be numerically stable.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z  input pre-activations (any shape)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        f(z) as described above applied elementwise to `Z`\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        ### s_i is a row of Z, so we want to take the max of every row of Z\n",
    "        m = np.max(Z, axis = 1)\n",
    "        m = m.reshape(Z.shape[0], 1)\n",
    "        num = np.exp(Z - m)\n",
    "        den = np.sum(num, axis = 1)\n",
    "        den = den.reshape(Z.shape[0], 1)\n",
    "        sigma = num / den\n",
    "        return sigma\n",
    "\n",
    "    def backward(self, Z: np.ndarray, dY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for softmax activation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z   input to `forward` method\n",
    "        dY  derivative of loss w.r.t. the output of this layer\n",
    "            same shape as `Z`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        derivative of loss w.r.t. input of this layer\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        dLdZ = np.zeros_like(Z)\n",
    "        S = self.forward(Z)\n",
    "        for i in range(0, S.shape[0]):\n",
    "            row = S[i].reshape((-1, 1))\n",
    "            J = -row @ row.T\n",
    "            diagonal = np.array([sigma * (1 - sigma) for sigma in row])\n",
    "            np.fill_diagonal(J, diagonal)\n",
    "            dLdZ[i] = dY[i] @ J\n",
    "        return dLdZ\n",
    "\n",
    "\n",
    "class ArcTan(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z):\n",
    "        return np.arctan(Z)\n",
    "\n",
    "    def backward(self, Z, dY):\n",
    "        return dY * 1 / (Z ** 2 + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbecbaef",
   "metadata": {},
   "source": [
    "# Losses.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914f1b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Sophia Sanborn\n",
    "Institution: UC Berkeley\n",
    "Date: Spring 2020\n",
    "Course: CS189/289A\n",
    "Website: github.com/sophiaas\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class Loss(ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "def initialize_loss(name: str) -> Loss:\n",
    "    if name == \"cross_entropy\":\n",
    "        return CrossEntropy(name)\n",
    "    elif name == \"l2\":\n",
    "        return L2(name)\n",
    "    else:\n",
    "        raise NotImplementedError(\"{} loss is not implemented\".format(name))\n",
    "\n",
    "\n",
    "class CrossEntropy(Loss):\n",
    "    \"\"\"Cross entropy loss function.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str) -> None:\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, Y: np.ndarray, Y_hat: np.ndarray) -> float:\n",
    "        return self.forward(Y, Y_hat)\n",
    "\n",
    "    def forward(self, Y: np.ndarray, Y_hat: np.ndarray) -> float:\n",
    "        \"\"\"Computes the loss for predictions `Y_hat` given one-hot encoded labels\n",
    "        `Y`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Y      one-hot encoded labels of shape (batch_size, num_classes)\n",
    "        Y_hat  model predictions in range (0, 1) of shape (batch_size, num_classes)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a single float representing the loss\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        m = Y.shape[0]\n",
    "        return (-1 / m) * np.sum(Y * np.log(Y_hat))\n",
    "\n",
    "    def backward(self, Y: np.ndarray, Y_hat: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass of cross-entropy loss.\n",
    "        NOTE: This is correct ONLY when the loss function is SoftMax.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Y      one-hot encoded labels of shape (batch_size, num_classes)\n",
    "        Y_hat  model predictions in range (0, 1) of shape (batch_size, num_classes)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        the derivative of the cross-entropy loss with respect to the vector of\n",
    "        predictions, `Y_hat`\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        m = Y.shape[0]\n",
    "        return - Y / (m * Y_hat)\n",
    "\n",
    "\n",
    "class L2(Loss):\n",
    "    \"\"\"Mean squared error loss.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str) -> None:\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, Y: np.ndarray, Y_hat: np.ndarray) -> float:\n",
    "        return self.forward(Y, Y_hat)\n",
    "\n",
    "    def forward(self, Y: np.ndarray, Y_hat: np.ndarray) -> float:\n",
    "        \"\"\"Compute the mean squared error loss for predictions `Y_hat` given\n",
    "        regression targets `Y`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Y      vector of regression targets of shape (batch_size, 1)\n",
    "        Y_hat  vector of predictions of shape (batch_size, 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a single float representing the loss\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        return ...\n",
    "\n",
    "    def backward(self, Y: np.ndarray, Y_hat: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for mean squared error loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Y      vector of regression targets of shape (batch_size, 1)\n",
    "        Y_hat  vector of predictions of shape (batch_size, 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        the derivative of the mean squared error with respect to the last layer\n",
    "        of the neural network\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        return ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d9144d",
   "metadata": {},
   "source": [
    "# Models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b825a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Sophia Sanborn\n",
    "Institution: UC Berkeley\n",
    "Date: Spring 2020\n",
    "Course: CS189/289A\n",
    "Website: github.com/sophiaas\n",
    "\"\"\"\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "\n",
    "from neural_networks.losses import initialize_loss\n",
    "from neural_networks.optimizers import initialize_optimizer\n",
    "from neural_networks.layers import initialize_layer\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# imports for typing only\n",
    "from neural_networks.utils.data_structures import AttrDict\n",
    "from neural_networks.datasets import Dataset\n",
    "from typing import Any, Dict, List, Sequence, Tuple\n",
    "\n",
    "\n",
    "def initialize_model(name, loss, layer_args, optimizer_args, logger=None, seed=None):\n",
    "\n",
    "    return NeuralNetwork(\n",
    "        loss=loss,\n",
    "        layer_args=layer_args,\n",
    "        optimizer_args=optimizer_args,\n",
    "        logger=logger,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "\n",
    "class NeuralNetwork(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        loss: str,\n",
    "        layer_args: Sequence[AttrDict],\n",
    "        optimizer_args: AttrDict,\n",
    "        logger=None,\n",
    "        seed: int = None,\n",
    "    ) -> None:\n",
    "\n",
    "        self.n_layers = len(layer_args)\n",
    "        self.layer_args = layer_args\n",
    "        self.logger = logger\n",
    "        self.epoch_log = {\"loss\": {}, \"error\": {}}\n",
    "\n",
    "        self.loss = initialize_loss(loss)\n",
    "        self.optimizer = initialize_optimizer(**optimizer_args)\n",
    "        self._initialize_layers(layer_args)\n",
    "\n",
    "    def _initialize_layers(self, layer_args: Sequence[AttrDict]) -> None:\n",
    "        self.layers = []\n",
    "        for l_arg in layer_args[:-1]:\n",
    "            l = initialize_layer(**l_arg)\n",
    "            self.layers.append(l)\n",
    "\n",
    "    def _log(self, loss: float, error: float, validation: bool = False) -> None:\n",
    "\n",
    "        if self.logger is not None:\n",
    "            if validation:\n",
    "\n",
    "                self.epoch_log[\"loss\"][\"validate\"] = round(loss, 4)\n",
    "                self.epoch_log[\"error\"][\"validate\"] = round(error, 4)\n",
    "                self.logger.push(self.epoch_log)\n",
    "                self.epoch_log = {\"loss\": {}, \"error\": {}}\n",
    "            else:\n",
    "                self.epoch_log[\"loss\"][\"train\"] = round(loss, 4)\n",
    "                self.epoch_log[\"error\"][\"train\"] = round(error, 4)\n",
    "\n",
    "    def save_parameters(self, epoch: int) -> None:\n",
    "        parameters = {}\n",
    "        for i, l in enumerate(self.layers):\n",
    "            parameters[i] = l.parameters\n",
    "        if self.logger is None:\n",
    "            raise ValueError(\"Must have a logger\")\n",
    "        else:\n",
    "            with open(\n",
    "                self.logger.save_dir + \"parameters_epoch{}\".format(epoch), \"wb\"\n",
    "            ) as f:\n",
    "                pickle.dump(parameters, f)\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"One forward pass through all the layers of the neural network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  design matrix whose must match the input shape required by the\n",
    "           first layer\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        forward pass output, matches the shape of the output of the last layer\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Iterate through the network's layers.\n",
    "        output = X\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "        return output\n",
    "\n",
    "    def backward(self, target: np.ndarray, out: np.ndarray) -> float:\n",
    "        \"\"\"One backward pass through all the layers of the neural network.\n",
    "        During this phase we calculate the gradients of the loss with respect to\n",
    "        each of the parameters of the entire neural network. Most of the heavy\n",
    "        lifting is done by the `backward` methods of the layers, so this method\n",
    "        should be relatively simple. Also make sure to compute the loss in this\n",
    "        method and NOT in `self.forward`.\n",
    "\n",
    "        Note: Both input arrays have the same shape.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        target  the targets we are trying to fit to (e.g., training labels)\n",
    "        out     the predictions of the model on training data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        the loss of the model given the training inputs and targets\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Compute the loss.\n",
    "        # Backpropagate through the network's layers.\n",
    "        L = self.loss.forward(target, out)\n",
    "        dLdY = self.loss.backward(target, out)\n",
    "        reverse_layers = self.layers[::-1]\n",
    "        for layer in reverse_layers:\n",
    "            dLdY = layer.backward(dLdY)\n",
    "        return L\n",
    "\n",
    "    def update(self, epoch: int) -> None:\n",
    "        \"\"\"One step of gradient update using the derivatives calculated by\n",
    "        `self.backward`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epoch  the epoch we are currently on\n",
    "        \"\"\"\n",
    "        param_log = {}\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            for param_name, param in layer.parameters.items():\n",
    "                if param_name != \"null\":  # FIXME: possible change needed to `is not`\n",
    "                    param_grad = layer.gradients[param_name]\n",
    "                    # Optimizer needs to keep track of layers\n",
    "                    delta = self.optimizer.update(\n",
    "                        param_name + str(i), param, param_grad, epoch\n",
    "                    )\n",
    "                    layer.parameters[param_name] -= delta\n",
    "                    if self.logger is not None:\n",
    "                        param_log[\"{}{}\".format(param_name, i)] = {}\n",
    "                        param_log[\"{}{}\".format(param_name, i)][\"max\"] = np.max(param)\n",
    "                        param_log[\"{}{}\".format(param_name, i)][\"min\"] = np.min(param)\n",
    "            layer.clear_gradients()\n",
    "        self.epoch_log[\"params\"] = param_log\n",
    "\n",
    "    def error(self, target: np.ndarray, out: np.ndarray) -> float:\n",
    "        \"\"\"Only calculate the error of the model's predictions given `target`.\n",
    "\n",
    "        For classification tasks,\n",
    "            error = 1 - accuracy\n",
    "\n",
    "        For regression tasks,\n",
    "            error = mean squared error\n",
    "\n",
    "        Note: Both input arrays have the same shape.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        target  the targets we are trying to fit to (e.g., training labels)\n",
    "        out     the predictions of the model on features corresponding to\n",
    "                `target`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        the error of the model given the training inputs and targets\n",
    "        \"\"\"\n",
    "        # classification error\n",
    "        if self.loss.name == \"cross_entropy\":\n",
    "            predictions = np.argmax(out, axis=1)\n",
    "            target_idxs = np.argmax(target, axis=1)\n",
    "            error = np.mean(predictions != target_idxs)\n",
    "\n",
    "        # regression error\n",
    "        elif self.loss.name == \"l2\":\n",
    "            error = np.mean((target - out) ** 2)\n",
    "\n",
    "        # Error!\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"Error for {} loss is not implemented\".format(self.loss)\n",
    "            )\n",
    "\n",
    "        return error\n",
    "\n",
    "    def train(self, dataset: Dataset, epochs: int) -> None:\n",
    "        \"\"\"Train the neural network on using the provided dataset for `epochs`\n",
    "        epochs. One epoch comprises one full pass through the entire dataset, or\n",
    "        in case of stochastic gradient descent, one epoch comprises seeing as\n",
    "        many samples from the dataset as there are elements in the dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset  training dataset\n",
    "        epochs   number of epochs to train for\n",
    "        \"\"\"\n",
    "        # Initialize output layer\n",
    "        args = self.layer_args[-1]\n",
    "        args[\"n_out\"] = dataset.out_dim\n",
    "        output_layer = initialize_layer(**args)\n",
    "        self.layers.append(output_layer)\n",
    "\n",
    "        for i in range(epochs):\n",
    "            training_loss = []\n",
    "            training_error = []\n",
    "            for _ in tqdm(range(dataset.train.samples_per_epoch)):\n",
    "                X, Y = dataset.train.sample()\n",
    "                Y_hat = self.forward(X)\n",
    "                L = self.backward(np.array(Y), np.array(Y_hat))\n",
    "                error = self.error(Y, Y_hat)\n",
    "                self.update(i)\n",
    "                training_loss.append(L)\n",
    "                training_error.append(error)\n",
    "            training_loss = np.mean(training_loss)\n",
    "            training_error = np.mean(training_error)\n",
    "            self._log(training_loss, training_error)\n",
    "\n",
    "            validation_loss = []\n",
    "            validation_error = []\n",
    "            for _ in range(dataset.validate.samples_per_epoch):\n",
    "                X, Y = dataset.validate.sample()\n",
    "                Y_hat = self.forward(X)\n",
    "                L = self.loss.forward(Y, Y_hat)\n",
    "                error = self.error(Y, Y_hat)\n",
    "                validation_loss.append(L)\n",
    "                validation_error.append(error)\n",
    "            validation_loss = np.mean(validation_loss)\n",
    "            validation_error = np.mean(validation_error)\n",
    "            self._log(validation_loss, validation_error, validation=True)\n",
    "\n",
    "            print(\"Example target: {}\".format(Y[0]))\n",
    "            print(\"Example prediction: {}\".format([round(x, 4) for x in Y_hat[0]]))\n",
    "            print(\n",
    "                \"Epoch {} Training Loss: {} Training Accuracy: {} Val Loss: {} Val Accuracy: {}\".format(\n",
    "                    i,\n",
    "                    round(training_loss, 4),\n",
    "                    round(1 - training_error, 4),\n",
    "                    round(validation_loss, 4),\n",
    "                    round(1 - validation_error, 4),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def test(\n",
    "        self, dataset: Dataset, save_predictions: bool = False\n",
    "    ) -> Dict[str, List[np.ndarray]]:\n",
    "        \"\"\"Makes predictions on the data in `datasets`, returning the loss, and\n",
    "        optionally returning the predictions and saving both.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset  test data\n",
    "        save_predictions  whether to calculate and save the predictions\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a dictionary containing the loss for each data point and optionally also\n",
    "        the prediction for each data point\n",
    "        \"\"\"\n",
    "        test_log = {\"loss\": [], \"error\": []}\n",
    "        if save_predictions:\n",
    "            test_log[\"prediction\"] = []\n",
    "        for _ in range(dataset.test.samples_per_epoch):\n",
    "            X, Y = dataset.test.sample()\n",
    "            Y_hat, L = self.predict(X, Y)\n",
    "            error = self.error(Y, Y_hat)\n",
    "            test_log[\"loss\"].append(L)\n",
    "            test_log[\"error\"].append(error)\n",
    "            if save_predictions:\n",
    "                test_log[\"prediction\"] += [x for x in Y_hat]\n",
    "        test_loss = np.mean(test_log[\"loss\"])\n",
    "        test_error = np.mean(test_log[\"error\"])\n",
    "        print(\n",
    "            \"Test Loss: {} Test Accuracy: {}\".format(\n",
    "                round(test_loss, 4), round(1 - test_error, 4)\n",
    "            )\n",
    "        )\n",
    "        if save_predictions:\n",
    "            with open(self.logger.save_dir + \"test_predictions.p\", \"wb\") as f:\n",
    "                pickle.dump(test_log, f)\n",
    "        return test_log\n",
    "\n",
    "    def test_kaggle(self, dataset: Dataset) -> Dict[str, List[np.ndarray]]:\n",
    "        \"\"\"Makes predictions on the data in `datasets`, returning the loss, and\n",
    "        optionally returning the predictions and saving both.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset  test data\n",
    "        save_predictions  whether to calculate and save the predictions\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a dictionary containing the loss for each data point and optionally also\n",
    "        the prediction for each data point\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for _ in range(dataset.test.samples_per_epoch):\n",
    "            X, Y = dataset.test.sample()\n",
    "            Y_hat, _ = self.predict(X, Y)\n",
    "            predictions += list(np.argmax(Y_hat, axis=1))\n",
    "        kaggle = pd.DataFrame(\n",
    "            OrderedDict({\"Id\": range(len(predictions)), \"Category\": predictions})\n",
    "        )\n",
    "        kaggle.to_csv(self.logger.save_dir + \"kaggle_predictions.csv\", index=False)\n",
    "        return kaggle\n",
    "\n",
    "    def predict(self, X: np.ndarray, Y: np.ndarray) -> Tuple[np.ndarray, float]:\n",
    "        \"\"\"Make a forward and backward pass to calculate the predictions and\n",
    "        loss of the neural network on the given data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input features\n",
    "        Y  targets (same length as `X`)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a tuple of the prediction and loss\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Do a forward pass. Maybe use a function you already wrote?\n",
    "        # Get the loss. Remember that the `backward` function returns the loss.\n",
    "        Y_hat = self.forward(X)\n",
    "        LOSS = self.backward(Y, Y_hat)\n",
    "        return Y_hat, LOSS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b99d6b",
   "metadata": {},
   "source": [
    "# Train_ffnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000796f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Step 1: Define layer arguments\n",
    "\n",
    "- Define the arguments for each layer in an attribute dictionary (AttrDict).\n",
    "- An attribute dictionary is exactly like a dictionary, except you can access the values as attributes rather than keys...for cleaner code :)\n",
    "- See layers.py for the arguments expected by each layer type.\n",
    "\"\"\"\n",
    "\n",
    "from neural_networks.utils.data_structures import AttrDict\n",
    "\n",
    "fc1 = AttrDict(\n",
    "    {\n",
    "        \"name\": \"fully_connected\",\n",
    "        \"activation\": \"relu\",\n",
    "        \"weight_init\": \"xavier_uniform\",\n",
    "        \"n_out\": 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "fc_out = AttrDict(\n",
    "    {\n",
    "        \"name\": \"fully_connected\",\n",
    "        \"activation\": \"softmax\",  # Softmax for last layer for classification\n",
    "        \"weight_init\": \"xavier_uniform\",\n",
    "        \"n_out\": None\n",
    "        # n_out is not defined for last layer. This will be set by the dataset.\n",
    "    }\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Step 2: Collect layer argument dictionaries into a list.\n",
    "\n",
    "- This defines the order of layers in the network.\n",
    "\"\"\"\n",
    "\n",
    "layer_args = [fc1, fc_out]\n",
    "\n",
    "\"\"\"\n",
    "Step 3: Define model, data, and logger arguments\n",
    "\n",
    "- The list of layer_args is passed to the model initializer.\n",
    "\"\"\"\n",
    "\n",
    "optimizer_args = AttrDict(\n",
    "    {\n",
    "        \"name\": \"SGD\",\n",
    "        \"lr\": 0.005,\n",
    "        \"lr_scheduler\": \"constant\",\n",
    "        \"lr_decay\": 0.99,\n",
    "        \"stage_length\": 1000,\n",
    "        \"staircase\": True,\n",
    "        \"clip_norm\": 1.0,\n",
    "        \"momentum\": 0.9,\n",
    "    }\n",
    ")\n",
    "\n",
    "model_args = AttrDict(\n",
    "    {\n",
    "        \"name\": \"feed_forward\",\n",
    "        \"loss\": \"cross_entropy\",\n",
    "        \"layer_args\": layer_args,\n",
    "        \"optimizer_args\": optimizer_args,\n",
    "        \"seed\": 0,\n",
    "    }\n",
    ")\n",
    "\n",
    "data_args = AttrDict(\n",
    "    {\n",
    "        \"name\": \"iris\",\n",
    "        \"batch_size\": 25,\n",
    "    }\n",
    ")\n",
    "\n",
    "log_args = AttrDict(\n",
    "    {\"save\": True, \"plot\": True, \"save_dir\": \"experiments/\",}\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Step 4: Set random seed\n",
    "\n",
    "Warning! Random seed must be set before importing other modules.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(model_args.seed)\n",
    "\n",
    "\"\"\"\n",
    "Step 5: Define model name for saving\n",
    "\"\"\"\n",
    "\n",
    "model_name = \"{}_{}layers_{}-lr{}_mom{}_seed{}\".format(\n",
    "    model_args.name,\n",
    "    len(layer_args),\n",
    "    fc1[\"n_out\"],\n",
    "    optimizer_args.lr,\n",
    "    optimizer_args.momentum,\n",
    "    model_args.seed,\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Step 6: Initialize logger, model, and dataset\n",
    "\n",
    "- model_name, model_args, and data_args are passed to the logger for saving\n",
    "- The logger is passed to the model.\n",
    "\"\"\"\n",
    "\n",
    "from neural_networks.models import initialize_model\n",
    "from neural_networks.datasets import initialize_dataset\n",
    "from neural_networks.logs import Logger\n",
    "\n",
    "\n",
    "logger = Logger(\n",
    "    model_name=model_name,\n",
    "    model_args=model_args,\n",
    "    data_args=data_args,\n",
    "    save=log_args.save,\n",
    "    plot=log_args.plot,\n",
    "    save_dir=log_args.save_dir,\n",
    ")\n",
    "\n",
    "\n",
    "model = initialize_model(\n",
    "    name=model_args.name,\n",
    "    loss=model_args.loss,\n",
    "    layer_args=model_args.layer_args,\n",
    "    optimizer_args=model_args.optimizer_args,\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "\n",
    "dataset = initialize_dataset(\n",
    "    name=data_args.name,\n",
    "    batch_size=data_args.batch_size,\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Step 7: Train model!\n",
    "\"\"\"\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "print(\n",
    "    \"Training {} neural network on {} with {} for {} epochs...\".format(\n",
    "        model_args.name, data_args.name, optimizer_args.name, epochs\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Optimizer:\")\n",
    "print(optimizer_args)\n",
    "\n",
    "model.train(dataset, epochs=epochs)\n",
    "model.test(dataset) # For Higgs, call test_kaggle() to generate the Kaggle file.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
